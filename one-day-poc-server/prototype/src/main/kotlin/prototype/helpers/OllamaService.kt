package prototype.helpers

import io.ktor.client.*
import io.ktor.client.engine.cio.*
import io.ktor.client.plugins.HttpTimeout
import io.ktor.client.request.*
import io.ktor.client.statement.*
import io.ktor.http.*
import kotlinx.serialization.Serializable
import kotlinx.serialization.encodeToString
import kotlinx.serialization.json.Json
import utils.environment.EnvironmentLoader

/**
 * This class encapsulates all parameters needed to make a request to the Ollama API.
 *
 * @property model The identifier of the language model to use (e.g., "codellama:13b")
 * @property prompt The text prompt to send to the language model
 * @property stream Whether to stream the response (false for complete response at once)
 * @property options Additional parameters to control model behavior such as temperature and sampling
 */
@Serializable
data class OllamaRequest(
    val model: String,
    val prompt: String,
    val stream: Boolean,
    val options: OllamaOptions = OllamaOptions(),
)

/**
 * Contains the complete response data returned by the Ollama service.
 *
 * @property model The identifier of the model that generated the response
 * @property created_at ISO 8601 timestamp when the response was generated
 * @property response The actual text generated by the language model
 * @property done Boolean indicating if the generation is complete
 * @property done_reason Reason code for completion (e.g., "stop", "length", "error")
 */
@Serializable
data class OllamaResponse(
    val model: String,
    val created_at: String,
    val response: String?,
    val done: Boolean,
    val done_reason: String,
)

/**
 * Configuration options for Ollama model generation.
 * These parameters control the behavior of the language model during text generation.
 *
 * @property temperature Controls randomness in generation. Higher values (e.g., 0.8) make output more random,
 *                      lower values (e.g., 0.2) make it more deterministic. Range: 0.0-1.0
 * @property top_k Limits token selection to the top K most likely tokens. Higher values allow more diversity.
 * @property top_p Nucleus sampling - consider tokens comprising the top_p probability mass. Range: 0.0-1.0
 * @property num_predict Maximum number of tokens to generate
 * @property stop List of strings that will stop generation when encountered
 */
@Serializable
data class OllamaOptions(
    val temperature: Double? = null,
    val top_k: Int? = null,
    val top_p: Double? = null,
    val num_predict: Int? = null,
    val stop: List<String>? = null,
)

/**
 * Wrapper class for OllamaResponse that includes extracted templates.
 * This approach keeps the original OllamaResponse class unchanged for backward compatibility.
 *
 * @property response The original OllamaResponse from Ollama
 * @property extractedTemplates List of code templates extracted from the LLM response
 */
@Serializable
data class EnhancedResponse(
    val response: OllamaResponse?,
    val extractedTemplates: List<String> = emptyList()
)


/**
 * Service for interacting with an Ollama instance
 */
object OllamaService {
    private val jsonParser = Json { ignoreUnknownKeys = true }
    private const val OLLAMA_PORT = 11434
    private val OLLAMA_HOST = EnvironmentLoader.get("OLLAMA_HOST").also { println(it) }
    private const val REQUEST_TIMEOUT_MILLIS = 600_000_000L
    private const val CONNECT_TIMEOUT_MILLIS = 30_000_000L
    private const val SOCKET_TIMEOUT_MILLIS = 600_000_000L

    var client =
        HttpClient(CIO) {
            install(HttpTimeout) {
                requestTimeoutMillis = REQUEST_TIMEOUT_MILLIS
                connectTimeoutMillis = CONNECT_TIMEOUT_MILLIS
                socketTimeoutMillis = SOCKET_TIMEOUT_MILLIS
            }
        }

    /**
     * Check if local Ollama instance is accessible.
     * @return true if Ollama is running, false otherwise.
     */
    suspend fun isOllamaRunning(): Boolean =
        runCatching {
            val response = client.get("http://$OLLAMA_HOST:$OLLAMA_PORT/")
            response.status == HttpStatusCode.OK
        }.getOrElse { false }

    /**
     * Sends a prompt to an LLM via Ollama and returns the generated response.
     *
     * @param request Input for the model formatted as an instance of [OllamaRequest].
     * @return Result object containing an instance of [OllamaResponse], or a failure with error message.
     */
    suspend fun generateResponse(request: OllamaRequest): Result<OllamaResponse?> {
        if (!isOllamaRunning()) {
            println("FUCK1")
            return Result.failure(Exception("Ollama is not running. Run: 'ollama serve' in terminal to start it."))
        }

        return try {
            Result.success(callOllama(request))
        } catch (e: Exception) {
            println("FUCK2")
            Result.failure(Exception("Failed to call Ollama: ${e.message}"))
        }
    }

    /**
     * Makes a call to Ollama and parses the response.
     *
     * @param request An instance of [OllamaRequest] containing the prompt and model for evaluation.
     * @return An instance of [OllamaResponse] containing the LLM's response, or null if parsing fails.
     * @throws Exception on network errors
     */
    private suspend fun callOllama(request: OllamaRequest): OllamaResponse? {
        val ollamaApiUrl = "http://$OLLAMA_HOST:$OLLAMA_PORT/api/generate"
        val response: HttpResponse =
            client.post(ollamaApiUrl) {
                header(HttpHeaders.ContentType, "application/json")
                setBody(jsonParser.encodeToString<OllamaRequest>(request))
            }

        if (response.status != HttpStatusCode.OK) {
            println("FUCK3")
            throw Exception("HTTP error: ${response.status}")
        }
        val responseText = response.bodyAsText()
        val ollamaResponse =
            runCatching {
                jsonParser.decodeFromString<OllamaResponse>(responseText)
            }.getOrElse {
                println("FUCK4")
                throw Exception("Failed to parse Ollama response")
            }

        // Only for debugging
        val jsonPrinter = Json { prettyPrint = true }
        println("Formatted JSON Response:\n" + jsonPrinter.encodeToString(ollamaResponse))
        return ollamaResponse
    }
}
